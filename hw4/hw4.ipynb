{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88702c70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f22503",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Homework 4 <br><br> Text Classification with Naive Bayes </center></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c870c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d604f2",
   "metadata": {},
   "source": [
    "The dataset that we will work with is a selection of posts from scikit-learn's ['20 newsgroups' dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset). We will be working with just two of the 20 newsgroup categories:  'comp.graphics' (computer graphics) and 'rec.motorcycles' (recreation motorcycles). \n",
    "\n",
    "The cell below loads the data from a pickle file. The variables are:\n",
    "\n",
    "+ `Xtrain`: A list of documents used for training\n",
    "+ `ytrain`: The category of each training document\n",
    "+ `Xtest`: A list of documents used for testing\n",
    "+ `ytest`: The category of each testing document\n",
    "+ `categories`: The set of all categories\n",
    "+ `vocabulary`: The feature set, ie words used in the model\n",
    "\n",
    "You should inspect the data to get a better understanding of its structure. For example, use `type` to see the data types of the variables and their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ab08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw4_text.pickle','rb') as file:\n",
    "    Xtrain, ytrain, Xtest, ytest, categories, vocabulary = pickle.load(file)\n",
    "    \n",
    "N = Xtrain.shape[0]   # number of documents in the training corpus\n",
    "K = len(categories)   # number of document categories (classes)\n",
    "D = len(vocabulary)   # number of words in the vocabulary (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c783be",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd9c4e",
   "metadata": {},
   "source": [
    "# 1. Find the number of training documents for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375aedf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_per_category = dict.fromkeys(categories, 0)\n",
    "\n",
    "# Your code here\n",
    "for category in docs_per_category.keys():\n",
    "    docs_per_category[category] = (ytrain == category).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a471a12",
   "metadata": {},
   "source": [
    "# 2. Create a bag-of-words representation for each document\n",
    "\n",
    "Our Naive Bayes algorithm will operate on a bag-of-words representation of each document. The first step is to write the `to_bow` method. \n",
    "\n",
    "The argument for this method is `doc`, which is an element of `X` (ie a string). It should return a `set` with the unique words that appear in both the document and the vocabulary. The comments in the method provide steps to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086baee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_bow(doc):\n",
    "    bow = set()\n",
    "    \n",
    "    # Split `doc` at spaces using the the string's `split` method. Obtain a list.                             \n",
    "    words = doc.split(' ')\n",
    "    \n",
    "    # Keep only unique words from the list, by casting it as a set. \n",
    "    bow = set(words)\n",
    "    \n",
    "    # From that set, keep only the ones that are present in the vocabulary.\n",
    "    bow = {word for word in bow if word in vocabulary}\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a0a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run `to_bow` on every document in `Xtrain`.\n",
    "Xtrain_bow = np.array([to_bow(doc) for doc in Xtrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f7e2c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c4113",
   "metadata": {},
   "source": [
    "# 3. Compute the document count for each word in each category\n",
    "\n",
    "To estimate probabilities for Naive Bayes, we will need to know, for each category and each word, the number of documents of the category that contain the word. Implement the `find_doc_counts_per_word_category` following the steps provided in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ea079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_doc_counts_per_word_category(categories,vocabulary,ytrain,Xtrain_bow):\n",
    "\n",
    "    # Initialize the dictionary\n",
    "    doc_counters = dict.fromkeys(categories) \n",
    "    for category in categories:\n",
    "        doc_counters[category]  = dict.fromkeys(vocabulary,0)\n",
    "\n",
    "    # Loop through categories and documents in that category. \n",
    "    # For each word in the vocabulary that is also in the document, increment the corresponding counter by 1. \n",
    "    for category in categories:\n",
    "        \n",
    "        # Filter Xtrain_bow and keep only the documents of this category\n",
    "        docs_in_category = []\n",
    "        for bow, labeled_category in zip(Xtrain_bow, ytrain):\n",
    "            if category == labeled_category:\n",
    "                docs_in_category.append(bow)\n",
    "\n",
    "        # For each document in the category, increment the appropriate counters\n",
    "        for doc in docs_in_category:\n",
    "            for word in doc:\n",
    "                doc_counters[category][word] += 1\n",
    "                    \n",
    "    return doc_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a9779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run `find_doc_counts_per_word_category`\n",
    "doccount_per_cat_and_word = find_doc_counts_per_word_category(categories,vocabulary,ytrain,Xtrain_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fda22",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228658c7",
   "metadata": {},
   "source": [
    "# 4. Find word frequencies per category\n",
    "\n",
    "Write the `compute_word_freq` method. \n",
    "\n",
    "The argument for this method is the Laplace smoothing factor `alpha`. It also uses global variables, including `doccount_per_cat_and_word`.\n",
    "\n",
    "For each category and word, compute the Laplace-smoothed ratio of the number of documents containing the word, to the total number of documents in the category. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. For each category and word in the vocabulary, compute $\\rho_{d,k}$  as\n",
    "\n",
    "$$\\rho_{d,k} = \\frac{(\\text{# documents of category $k$ that contain word $d$}) + \\alpha}{(\\text{# documents of category $k$})+\\alpha K}$$\n",
    "\n",
    "Store it in `wordfreq[category][word]`.\n",
    "\n",
    "2. For each category, compute $\\rho_k$  as\n",
    "    \n",
    "$$\\rho_k = \\frac{\\text{# documents of category $k$}}{\\text{Total # documents}}$$\n",
    "\n",
    "Store it in `catfreq[category]`.\n",
    "\n",
    "Notice that we are not applying Laplace smoothing to the category frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864d247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_word_freq(alpha):\n",
    "    K = len(vocabulary)\n",
    "\n",
    "    # Initialize `wordfreq` and `catfreq`\n",
    "    wordfreq = dict.fromkeys(categories)\n",
    "    for category in categories:\n",
    "        wordfreq[category] = dict.fromkeys(vocabulary)\n",
    "    catfreq = dict.fromkeys(categories)\n",
    "    \n",
    "    # Step 1, compute wordfreq\n",
    "    for category in categories:\n",
    "        for word, word_freq in doccount_per_cat_and_word[category].items():\n",
    "            wordfreq[category][word] = (word_freq + alpha) / (docs_per_category[category] + alpha*D)\n",
    "    \n",
    "    # Step 2, compute catfreq\n",
    "    for category in categories:\n",
    "        catfreq[category] = docs_per_category[category] / len(Xtrain)\n",
    "    \n",
    "    return wordfreq, catfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80b3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run `compute_word_log_freq` with $\\alpha=0.01$.\n",
    "wordfreq, catfreq = compute_word_freq(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c165727",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7a8ef",
   "metadata": {},
   "source": [
    "# 5. Write the Naive Bayes prediction function.\n",
    "\n",
    "Compute the Naive Bayes prediction of the category for the given test document `xtest`. \n",
    "\n",
    "The arguments for this method are \n",
    "+ xtest: a single test document as a string.\n",
    "+ wordfreq, catfreq: the ratios computed in the previous step (with $\\alpha=0.1$)\n",
    "\n",
    "The steps for are:\n",
    "1. Find the BOW representation of `xtest`.\n",
    "\n",
    "2. Use the dictionary `score_cat` to store the score for each of the categories.\n",
    "\n",
    "3. Loop through categories, for each one compute its score with\n",
    "\n",
    "$$\\log\\rho_k+ \\sum_{d:\\:x_d=1} \\log\\rho_{d,k} + \\sum_{d:\\:x_d=0} \\log(1-\\rho_{d,k})$$\n",
    "\n",
    "Here $x_d$ is the $d$'th word in `xtest`\n",
    "\n",
    "4. Return the category with the highest score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86665e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(Xtest, wordfreq, catfreq):\n",
    "\n",
    "    # 1. Find the BOW representation of Xtest.\n",
    "    Xtest_bow = to_bow(Xtest)\n",
    "    \n",
    "    # 2. Use a dictionary to store the score for each of the categories.\n",
    "    score_cat = dict.fromkeys(categories,0)\n",
    "\n",
    "    # 3. Loop through categories, for each one compute its score, and save it in score_cat.\n",
    "    for category in categories:\n",
    "        wordfreq, catfreq = compute_word_freq(0.1)\n",
    "        word_sum = 0\n",
    "        for word in wordfreq[category]:\n",
    "            rho_k = catfreq[category]\n",
    "            rho_dk = wordfreq[category][word]\n",
    "            word_sum += np.log(rho_dk) if word in Xtest_bow else np.log(1 - rho_dk)\n",
    "        score_cat[category] = np.log(catfreq[category]) + word_sum\n",
    "        \n",
    "    # 4. Return the category with the highest score.\n",
    "    return max(score_cat, key=score_cat.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04061c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b185c76",
   "metadata": {},
   "source": [
    "# 6. Compute accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf8a99",
   "metadata": {},
   "source": [
    "Accuracy is defined as the number of correct predictions, divided by the total number of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c215cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(Xin, yin, wordfreq, catfreq):\n",
    "\n",
    "    correct = 0\n",
    "    \n",
    "    # count the number of correct predictions\n",
    "    for i in range(len(Xin)):\n",
    "        ...\n",
    "        \n",
    "    return correct/len(Xin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ee4ce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262e970",
   "metadata": {},
   "source": [
    "# 7. Compute the training and testing errors for a range of $\\alpha$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d85341",
   "metadata": {},
   "source": [
    "### 7.1. Train the model and compute its test accuracy for logarithmically spaced values of $\\alpha$ ranging from $10^{-5}$ to $10^1$\n",
    "\n",
    "Here 'training the model' means computing the Laplace-smoothed document frequencies with `compute_word_freq`. Do this for a range of $\\alpha$'s and store their corresponding accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2881069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-5,1,20)\n",
    "acc = np.empty(len(alphas))\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265213a",
   "metadata": {},
   "source": [
    "### 7.2. Plot the accuracies as a function of $\\alpha$ using `plt.semilogx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43e4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042f9a7",
   "metadata": {},
   "source": [
    "### 7.3. What is the optimal $\\alpha$ and its corresponding accuracy?  [Hint](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d13a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_acc = ...\n",
    "best_alpha = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d7b0e6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d7fd0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3302bd8",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b8b5ce4b1bd0cdb09a48c826d4154f25cb98d27fcdd75ace86cf123225b5557"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xtrain_bow[23]=={'a',\n...  'and',\n...  'answer',\n...  'bad',\n...  'but',\n...  'find',\n...  'for',\n...  'here',\n...  'i',\n...  'if',\n...  'in',\n...  'is',\n...  'it',\n...  'maybe',\n...  'my',\n...  'not',\n...  'of',\n...  'posting',\n...  'sorry',\n...  'the',\n...  'thing',\n...  'this',\n...  'try',\n...  'use',\n...  'will',\n...  'work',\n...  'your'}\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> doccount_per_cat_and_word['rec.motorcycles']['maybe']==1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> doccount_per_cat_and_word['comp.graphics']['each']==1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> doccount_per_cat_and_word['comp.graphics']['animation']==2\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(wordfreq['rec.motorcycles']['simply'],0.024950592885375492, atol=1e-2) \\\n... or \\\n... np.isclose(wordfreq['rec.motorcycles']['simply'],  0.028840662478583665, atol=1e-2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(wordfreq['rec.motorcycles']['wondering'], 0.00024703557312252963, atol=1e-2) \\\n... or \\\n... np.isclose(wordfreq['rec.motorcycles']['wondering'], 0.00028555111364934324, atol=1e-2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(wordfreq['rec.motorcycles']['sun'], 0.024950592885375492, atol=1e-2) \\\n... or \\\n... np.isclose(wordfreq['rec.motorcycles']['sun'], 0.028840662478583665, atol=1e-2) \nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(wordfreq['comp.graphics']['video'],0.098753280839895, atol=1e-2) \\\n... or \\\n... np.isclose(wordfreq['comp.graphics']['video'],0.12030375699440447, atol=1e-2) \nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(wordfreq['comp.graphics']['when'], 0.098753280839895, atol=1e-2) \\\n... or \\\n... np.isclose(wordfreq['comp.graphics']['when'], 0.12030375699440447, atol=1e-2) \nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(wordfreq['comp.graphics']['ftp'], 0.00032808398950131233, atol=1e-2) \\\n... or \\\n... np.isclose(wordfreq['comp.graphics']['ftp'], 0.0003996802557953637, atol=1e-2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(catfreq['rec.motorcycles'], 0.5833333333333334, atol=1e-2) \\\n... or \\\n... np.isclose(catfreq['rec.motorcycles'], 0.5833333333333334, atol=1e-2)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> predict(Xtest[0], wordfreq, catfreq)=='rec.motorcycles'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> predict(Xtest[13], wordfreq, catfreq)=='rec.motorcycles'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # alpha D or alpha K\n>>> np.isclose(compute_accuracy(Xtest, ytest, wordfreq, catfreq),0.8508771929824561,atol=1e-3) \\\n... or \\\n... np.isclose(compute_accuracy(Xtest, ytest, wordfreq, catfreq),0.8543859649122807,atol=1e-3)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7p1": {
     "name": "q7p1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # alpha D or alpha K\n>>> np.all(np.isclose(acc[:3],[0.83859649, 0.84035088, 0.84210526])) \\\n... or \\\n... np.all(np.isclose(acc[:3],[0.84035088, 0.84210526, 0.84385965]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7p3": {
     "name": "q7p3",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
